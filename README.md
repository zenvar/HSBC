## HSBC Career Crawler - A Gen-AI Powered Jobs Posting Scraper

**A crawler system that periodically fetchs job postings details from HSBC Career websites, and sends notifications summarized by the Generative AI(Large Language Model,here we used the Google Gemini flash) to a Telegram channel via the Telegram bot.**

### Architecture Design

This project adopts a modular design and includes the following modules:

* **crawler:** Responsible for scraping job posting data.
    * `spiders`: Contains Scrapy spider files defining scraping rules and data extraction logic.
    * `pipelines`: Processes scraped data, including cleaning, deduplication, etc.
    * `settings`: Scrapy settings file, configuring crawler parameters, middleware, etc.
* **db:** Handles storage of job posting information.
    * `database`: Database operation class, encapsulating database connection, query, and insertion operations.
* **notifier:** Handles sending new job notifications.
    * `telegram`: Telegram notification class, encapsulating Telegram message sending functionality.
* **scheduler:** Responsible for scheduling the periodic execution of the crawler.
    * `job_scheduler`: Scheduler class, using APScheduler to schedule tasks.
* **utils:** Utility module containing common helper functions.
    * `logger`: Logging class, encapsulating logging functionality.
* **config:** Configuration module for storing and managing various configuration information.
    * `config.py`: Configuration reading and management class, responsible for reading configuration files for different environments.
    * `dev.ini` and `prod.ini`: Example configuration files for development and production environments, respectively.

### Implementation

* **Scraping Logic:** The crawler uses the Scrapy framework, leveraging CSS selectors to extract target data. Data from listing pages is passed to detail page scraping functions using the `meta` parameter.
* **Incremental Crawling:** The database records the latest scraped job URLs, and only new job postings are scraped each time.
* **Data Cleaning:** Regular expressions and string methods are used to clean job descriptions, removing unnecessary spaces, line breaks, and special characters.
* **Data Deduplication:** Database primary key constraints (ID) prevent duplicate data insertion. When duplicate IDs are encountered, a new ID is generated by concatenating the original ID with a timestamp, and the `reopened` field is set to 1, indicating that the job posting has been reopened.
* **Notification Function:** New job postings are sent as notifications using a Telegram Bot.
* **Scheduled Tasks:** APScheduler is used to schedule the crawler to run periodically. The scraping interval can be configured in the configuration file.

### Deployment

This project uses Docker for containerization and Docker Compose for multi-container management.

**Deployment Steps:**

1. **Build Docker Image:**

   ```bash
   docker build -t job-crawler .
   ```

2. **Run Docker Container (using docker command):**

   ```bash
   docker run -d -v ./data:/HSBC/data job-crawler
   ```

3. **Run Docker Container (using docker-compose):**

   ```bash
   docker-compose up -d
   ```

**Configuration Files:**

Configure the following information in the `config/dev.ini` or `prod.ini` files:

*   Database path (`database.path`)
*   Crawler URL (`crawler.list_url`)
*   Telegram Bot Token (`crawler.telegram_bot_token`)
*   Telegram Chat ID (`crawler.telegram_chat_id`)
*   Scraping interval (`scheduler.interval`)

**Note:**

*   Replace placeholders in configuration files with your actual configuration information.
*   Create a Telegram Bot and obtain its Token and Chat ID.
*   Database files will be stored in the `data` directory on the host machine. Ensure that the directory exists and has write permissions.


### Usage

1. **Configure Environment:**

   * Install Python 3.9 and required dependencies (see `requirements.txt`).
   * Create a Telegram Bot and obtain its Token and Chat ID.
   * Configure the database path and crawler URL.

2. **Run the Crawler:**

   * Run `python scheduler/job_scheduler.py` in the project root directory to start the scheduled task.

3. **View Logs:**

   * Use the command `docker logs job-crawler` to view the container's log output.

### Disclaimer

*   This project is for learning and reference purposes only, please do not use it for illegal purposes.
*   Please comply with the robots.txt protocol and the website's terms of service when crawling websites.
*   Do not crawl websites too frequently to avoid putting pressure on them.

Hope this provides a clear understanding of the project!
